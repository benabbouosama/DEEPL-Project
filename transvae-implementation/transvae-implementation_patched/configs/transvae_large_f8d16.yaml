# TransVAE-Large f16d32 Configuration

model:
  variant: large
  compression_ratio: 8     # f
  latent_dim: 16      # d
  
  # Architecture settings
  depths: [3, 3, 6, 8]
  base_dims: [192, 384, 768, 1536]
  mlp_ratio: 1.0
  head_dim: 64
  
  # Key components
  use_rope: true
  use_conv_ffn: true
  use_dc_path: true

training:
  # Optimization
  batch_size: 256
  learning_rate: 1.0e-4
  num_epochs: 100
  warmup_steps: 10000
  grad_clip: 1.0
  optimizer: adamw
  weight_decay: 0.0
  
  # Data
  resolution: 256
  num_workers: 8
  
  # Mixed precision
  mixed_precision: true
  gradient_checkpointing: false

losses:
  # Stage 1: Reconstruction training (100 epochs)
  l1_weight: 1.0
  lpips_weight: 1.0
  kl_weight: 1.0e-8
  vf_weight: 0.1
  gan_weight: 0.0
  use_gan: false
  
  Stage 2: GAN refinement (10 epochs)
  # Uncomment for stage 2:
  gan_weight: 0.05
  use_gan: true
  freeze_encoder: true

evaluation:
  metrics: [psnr, ssim, lpips, rfid]
  eval_freq: 1000
  save_freq: 5000

# Notes:
# - For stage 1, train encoder+decoder for 100 epochs with L1, LPIPS, KL, VF losses
# - For stage 2, freeze encoder, train decoder for 10 epochs with GAN loss added
# - Total training: ~200k steps on ImageNet-1k at 256x256
